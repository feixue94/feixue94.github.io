<!DOCTYPE HTML>
<html style="background-color:whitesmoke; color:black;" lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Fei Xue</title>
  
  <meta name="author" content="Fei Xue">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/University_Crest.png">
  
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PB26JZTCNC"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-PB26JZTCNC');
  </script>
  
</head>

<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Fei Xue</name>
              </p>
              <p>
                I am a PhD student in the Computer Vision Group at the University of Cambridge,
                supervised by <a href="https://mi.eng.cam.ac.uk/~cipolla/">Prof. Roberto Cipolla</a> and <a href="http://mi.eng.cam.ac.uk/~ib255/">Dr. Ignas Budvytis</a>.
              </p>
              <p>
                My research interests lie in large-scale 3D reconstruction and localization for autonomous driving, robotics, and AR/VR. I am especially interested in efficient and accurate pose estimation by introducing multimodality signas, semantics, and geometric constraints.
              </p>
              <p>
                Prior to my PhD, I obtained the Bachelor and Master degrees from Peking University under the supervision of <a href="https://www.cis.pku.edu.cn/info/1177/1379.htm">Prof. Hongbin Zha </a>.
                As a student, I have been fortunate to be an intern at UiSee, SenseTime, and NVIDIA.
              </p>
              <p style="text-align:center">
                <a href="mailto:fx221@cam.ac.uk">Email</a> &nbsp/&nbsp
                <a href="https://github.com/feixue94">Github</a> &nbsp/&nbsp
                <a href="https://twitter.com/FeiXue94">Twitter</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=G2sYDPkAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/fei-xue-6b4a03139/">LinkedIn </a> &nbsp/&nbsp
                <a href="data/RESUME_FeiXue.pdf">CV</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/fei.png"><img style="width:60%;max-width:60%" alt="profile photo" src="images/fei.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <!-- <h2>Background</h2> -->

        <h2>News</h2>
        <ul>
          <li>
            2024.04 &nbsp; Release of PRAM and VRS-NeRF <a href="https://feixue94.github.io/pram-project/" target="_blank"></a>!
        </li>

        <li>
            2023.05 &nbsp; Two papers are accepted by <a href="https://cvpr2023.thecvf.com/" target="_blank">CVPR 2023</a>!
        </li>
        <li> 
            2022.05 &nbsp; One paper is accepted by <a href="https://cvpr2021.thecvf.com/" target="_blank">CVPR 2021</a>!
        </li>   
<li> 
            2021.10 &nbsp; One paper is accepted by TPAMI 2022!
        </li>   
        <li> 
            2020.05 &nbsp; Two papers are accepted by <a href="https://cvpr2020.thecvf.com/" target="_blank">CVPR 2020</a>!
        </li>   
        <li> 
            2020.05 &nbsp; Two papers are accepted by <a href="https://iccv2019.thecvf.com/" target="_blank">ICCV 2019</a>!
        </li>     
        <li> 
            2019.05 &nbsp; One paper is accepted by <a href="https://cvpr2019.thecvf.com/" target="_blank">CVPR 2019</a> as oral!
        </li>    
        <li> 
            2018.06 &nbsp; Two papers are accepted by <a href="https://www.accv2018.net/" target="_blank">ACCV 2018</a>!
        </li>   
        </ul>

        <h2>Academic Activities</h2>
        <ul>
          <li>Conference reviewer of: CVPR, ICCV, ECCV, NeurIPS, ICML, ICLR. </li>
          <li>Journal reviewer of: TPAMI, PR. </li>
      </ul>

        <h2>Research (<a href="https://scholar.google.com/citations?user=G2sYDPkAAAAJ&hl=en"> Google Scholar </a>)</h2>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="papers/pram.png" alt="arxiv2024" style="border-style: none" width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>PRAM: Place Recognition Anywhere Model for Efficient Visual Localization</papertitle>
            <br>
            <strong>Fei Xue</strong>,
            <a href="http://mi.eng.cam.ac.uk/~ib255/">Ignas Budvytis</a>,
            <a href="https://mi.eng.cam.ac.uk/~cipolla/">Roberto Cipolla</a>
            <br>
            <em>Arxiv 2024</em>
            <br>
            <a href="https://arxiv.org/pdf/2404.07785.pdf">Paper</a> &nbsp/&nbsp
            <a href="https://github.com/feixue94/pram">Code</a> &nbsp/&nbsp
            <a href="https://feixue94.github.io/pram-project/">Project</a>
            <br>
            <p>
              We propose Place Recognition Anywhere Model (PRAM) for efficient large-scale localization which automatically defines landmarks in any scenes and recognizes these landmarks for both coarse and fine localization. Previous works of semantic-aware features <a href="https://github.com/feixue94/sfd2">SFD2</a> and geometric-aware matcher <a href="https://github.com/feixue94/imp-release">IMP</a> are used.
            </p>
          </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="papers/vrs-nerf.png" alt="arxiv2024" style="border-style: none" width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>VRS-NeRF: Visual Relocalization with Sparse Neural Radiance Field</papertitle>
            <br>
            <strong>Fei Xue</strong>,
            <a href="http://mi.eng.cam.ac.uk/~ib255/">Ignas Budvytis</a>,
            Daniel Olmeda Reino,
            <a href="https://mi.eng.cam.ac.uk/~cipolla/">Roberto Cipolla</a>
            <br>
            <em>Arxiv 2024</em>
            <br>
            <!-- <a href="https://arxiv.org/pdf/2404.07785.pdf">Paper</a> &nbsp/&nbsp -->
            <a href="https://github.com/feixue94/vrs-nerf">Code</a>
            <!-- <a href="https://feixue94.github.io/pram-project/">Project</a> -->
            <br>
            <p>
              A NeRF-based localization pipeline with sparse rendering for high efficiency. Previous works semantic-aware features <a href="https://github.com/feixue94/sfd2">SFD2</a> and geometric-aware matcher <a href="https://github.com/feixue94/imp-release">IMP</a> are used.
            </p>
          </td>
          </tr>

          <!-- IMP -->
          <tr>
            <!-- <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="papers/imp.png" alt="CVPR2023" style="border-style: none" width="300">
            </td> -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="papers/IMP.gif"><img src="papers/IMP.gif" alt="imp2023gif" style="border-style: none" width="320"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>IMP: Iterative Matching and Pose Estimation with Adaptive Pooling</papertitle>
            <br>
            <strong>Fei Xue</strong>,
            <a href="http://mi.eng.cam.ac.uk/~ib255/">Ignas Budvytis</a>,
            <a href="https://mi.eng.cam.ac.uk/~cipolla/">Roberto Cipolla</a>
            <br>
            <em>CVPR 2023</em>
            <br>
            <a href="https://arxiv.org/abs/2304.14837">Paper</a> &nbsp/&nbsp
            <a href="https://github.com/feixue94/imp-release">Code</a> &nbsp/&nbsp
            <a href="https://www.youtube.com/watch?v=yH3d-AzyTGo">Video</a> &nbsp/&nbsp
            <a href="papers/IMP-CVPR2023/slides.pdf">Slides</a> &nbsp/&nbsp
            <a href="papers/IMP-CVPR2023/poster.pdf">Poster</a>
            <!-- <a href="https://feixue94.github.io/pram-project/">Project</a> -->
            <br>
            <p>
              We mebed geometric constraint into graph-based matcher (e.g. SuperGlue) to make it work more accurate and robust in challenging conditions (e.g. large viewpoint changes, repetitve textures). Attention scores are used to remove useless keypoints progressively to achieve higher effciency.
            </p>
          </td>
          </tr>

          <!-- SFD2-CVPR2023 -->
          <tr>
            <!-- <td style="padding:20px;width:25%;vertical-align:middle"> -->
              <!-- <img src="papers/sfd2.png" alt="CVPR2023" style="border-style: none" width="300"> -->
            <!-- </td> -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="papers/SFD2.gif"><img src="papers/SFD2.gif" alt="sfd22023gif" style="border-style: none" width="320"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>SFD2: Semantic-guided Feature Detection and Description</papertitle>
            <br>
            <strong>Fei Xue</strong>,
            <a href="http://mi.eng.cam.ac.uk/~ib255/">Ignas Budvytis</a>,
            <a href="https://mi.eng.cam.ac.uk/~cipolla/">Roberto Cipolla</a>
            <br>
            <em>CVPR 2023</em>
            <br>
            <a href="https://arxiv.org/abs/2304.14845">Paper</a> &nbsp/&nbsp
            <a href="https://github.com/feixue94/sfd2">Code</a> &nbsp/&nbsp
            <a href="https://www.youtube.com/watch?v=WzVXG5pyrgo">Video</a> &nbsp/&nbsp
            <a href="papers/SFD2-CVPR2023/slides.pdf">Slides</a> &nbsp/&nbsp
            <a href="papers/SFD2-CVPR2023/poster.pdf">Poster</a>
            <!-- <a href="https://feixue94.github.io/pram-project/">Project</a> -->
            <br>
            <p>
              Semantics are very useful for local feature detection and description especially for long-term tasks. However, explicit usage of semantics requires segmentation networks and has severe semantic uncertainties. In this paper, we implicitly embed semantics into detection and description to detect robust keypoints and extract semantically-augmented descriptors.
            </p>
          </td>
          </tr>

          <!-- LBR-CVPR2022 -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="papers/lbr.png" alt="CVPR2022" style="border-style: none" width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Efficient Large-scale Localization by Global Instance Recognition</papertitle>
            <br>
            <strong>Fei Xue</strong>,
            <a href="http://mi.eng.cam.ac.uk/~ib255/">Ignas Budvytis</a>,
            Daniel Olmeda Reino,
            <a href="https://mi.eng.cam.ac.uk/~cipolla/">Roberto Cipolla</a>
            <br>
            <em>CVPR 2022</em>
            <br>
            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Xue_Efficient_Large-Scale_Localization_by_Global_Instance_Recognition_CVPR_2022_paper.pdf">Paper</a> &nbsp/&nbsp
            <a href="https://github.com/feixue94/lbr">Code</a> &nbsp/&nbsp
            <a href="https://www.youtube.com/watch?v=w5b8RA3BnRc">Video</a> &nbsp/&nbsp
            <a href="papers/LBR-CVPR2022/slides.pdf">Slides</a> &nbsp/&nbsp
            <a href="papers/LBR-CVPR2022/poster.pdf">Poster</a>
            <!-- <a href="https://feixue94.github.io/pram-project/">Project</a> -->
            <br>
            <p>
              Our first trial on large-scale localization by recognition. We define global instances on building facades which are discriminative for coarse localization and robust to appearance changes. At test time, we recognize these global instances and use them for city-scale localization.
            </p>
          </td>
          </tr>

          <!-- VOMachine -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="papers/adaptive.png" alt="CVPR2022" style="border-style: none" width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Deep Visual Odometry with Adaptive Memory</papertitle>
            <br>
            <strong>Fei Xue</strong>,
            Xin Wang, Junqiu Wang, Hongbin Zha
            <br>
            <em>TPAMI 2022</em>
            <br>
            <a href="https://arxiv.org/abs/2008.01655">Paper</a> &nbsp/&nbsp
            <a href="https://feixue94.github.io/">Code</a> &nbsp/&nbsp
            <!-- <a href="https://feixue94.github.io/pram-project/">Project</a> -->
            <br>
            <p>
              An end-to-end VO system with tracking, remembering and refining components. It works impressively well in autonomous driving and robotics scrnarios.
            </p>
          </td>
          </tr>

                    <!-- LBR-CVPR2022 -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="papers/lbr.png" alt="CVPR2022" style="border-style: none" width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Efficient Large-scale Localization by Global Instance Recognition</papertitle>
            <br>
            <strong>Fei Xue</strong>,
            <a href="http://mi.eng.cam.ac.uk/~ib255/">Ignas Budvytis</a>,
            Daniel Olmeda Reino,
            <a href="https://mi.eng.cam.ac.uk/~cipolla/">Roberto Cipolla</a>
            <br>
            <em>CVPR 2022</em>
            <br>
            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Xue_Efficient_Large-Scale_Localization_by_Global_Instance_Recognition_CVPR_2022_paper.pdf">Paper</a> &nbsp/&nbsp
            <a href="https://github.com/feixue94/lbr">Code</a> &nbsp/&nbsp
            <a href="https://www.youtube.com/watch?v=w5b8RA3BnRc">Video</a> &nbsp/&nbsp
            <a href="papers/LBR-CVPR2022/slides.pdf">Slides</a> &nbsp/&nbsp
            <a href="papers/LBR-CVPR2022/poster.pdf">Poster</a>
            <!-- <a href="https://feixue94.github.io/pram-project/">Project</a> -->
            <br>
            <p>
              Our first trial on large-scale localization by recognition. We define global instances on building facades which are discriminative for coarse localization and robust to appearance changes. At test time, we recognize these global instances and use them for city-scale localization.
            </p>
          </td>
          </tr>

          <!-- LBR-GLnet2020 -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="papers/glnet.png" alt="CVPR2022" style="border-style: none" width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Learning Multi-view Camera Relocalization with Graph Neural Networks</papertitle>
            <br>
            <strong>Fei Xue</strong>,
            Xin Wu, Shaojun Cai, Junqiu Wang
            <br>
            <em>CVPR 2020</em>
            <br>
            <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Xue_Learning_Multi-View_Camera_Relocalization_With_Graph_Neural_Networks_CVPR_2020_paper.pdf">Paper</a> &nbsp/&nbsp
            <a href="https://github.com/feixue94/glnet-release">Code</a> 
            <!-- <a href="https://www.youtube.com/watch?v=w5b8RA3BnRc">Video</a> &nbsp/&nbsp -->
            <!-- <a href="papers/LBR-CVPR2022/slides.pdf">Slides</a> &nbsp/&nbsp -->
            <!-- <a href="papers/LBR-CVPR2022/poster.pdf">Poster</a> -->
            <!-- <a href="https://feixue94.github.io/pram-project/">Project</a> -->
            <br>
            <p>
             An end-to-end localization frameowrk which formulates multi-view inputs as a graph and leverages GNN for multi-view information fusion. It works very well in scenarios where a single-view input leads to errors due to similar structures etc.
            </p>
          </td>
          </tr>

          <!-- Online Adaptation -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="papers/online.png" alt="CVPR2020" style="border-style: none" width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Self-Supervised Deep Visual Odometry with Online Adaptation</papertitle>
            <br>
            Shunkai Li, Xin Wang, Yingdian Cao
            <strong>Fei Xue</strong>,
            <a href="https://zikeyan.github.io/">Zike Yan</a>, 
            Hongbin Zha
            <br>
            <em>CVPR 2020 (oral)</em>
            <br>
            <a href="https://arxiv.org/pdf/2005.06136.pdf">Paper</a> &nbsp/&nbsp
            <a href="https://feixue94.github.io">Code</a> 
            <!-- <a href="https://www.youtube.com/watch?v=w5b8RA3BnRc">Video</a> &nbsp/&nbsp -->
            <!-- <a href="papers/LBR-CVPR2022/slides.pdf">Slides</a> &nbsp/&nbsp -->
            <!-- <a href="papers/LBR-CVPR2022/poster.pdf">Poster</a> -->
            <!-- <a href="https://feixue94.github.io/pram-project/">Project</a> -->
            <br>
            <p>
             An end-to-end VO framework with online adaptation at test time to enhance its ability of working in more general environments.
            </p>
          </td>
          </tr>

        <!-- LSG -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="papers/lsg.png" alt="CVPR2020" style="border-style: none" width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Local Supports Global: Deep Camera Relocalization with Sequencen Enhancement</papertitle>
            <br>
            <strong>Fei Xue</strong>,
            Xin Wang, Zike Yan, Qiuyuan Wang, Junqiu Wang, Hongbin Zha
            <br>
            <em>ICCV 2019</em>
            <br>
            <a href="https://arxiv.org/pdf/1908.04391.pdf">Paper</a> &nbsp/&nbsp
            <a href="https://feixue94.github.io">Code</a> 
            <!-- <a href="https://www.youtube.com/watch?v=w5b8RA3BnRc">Video</a> &nbsp/&nbsp -->
            <!-- <a href="papers/LBR-CVPR2022/slides.pdf">Slides</a> &nbsp/&nbsp -->
            <!-- <a href="papers/LBR-CVPR2022/poster.pdf">Poster</a> -->
            <!-- <a href="https://feixue94.github.io/pram-project/">Project</a> -->
            <br>
            <!-- <p> -->
             <!-- An end-to-end VO framework with online adaptation at test time to enhance its ability of working in more general environments. -->
            <!-- </p> -->
          </td>
          </tr>


          <!-- Sequential -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="papers/sequential.png" alt="CVPR2020" style="border-style: none" width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Sequential Adversarial Learning for Self-Supervised Deep Visual Odometry</papertitle>
            <br>
            Shunkai Li,
            <strong>Fei Xue</strong>,
            <a href="https://zikeyan.github.io/">Zike Yan</a>, 
            Xin Wang, Zike Yan, Hongbin Zha
            <br>
            <em>ICCV 2019</em>
            <br>
            <a href="https://arxiv.org/pdf/1908.08704.pdf">Paper</a> &nbsp/&nbsp
            <a href="https://feixue94.github.io">Code</a> 
            <!-- <a href="https://www.youtube.com/watch?v=w5b8RA3BnRc">Video</a> &nbsp/&nbsp -->
            <!-- <a href="papers/LBR-CVPR2022/slides.pdf">Slides</a> &nbsp/&nbsp -->
            <!-- <a href="papers/LBR-CVPR2022/poster.pdf">Poster</a> -->
            <!-- <a href="https://feixue94.github.io/pram-project/">Project</a> -->
            <br>
          </td>
          </tr>

          <!-- BeyondTracking -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="papers/beyond.png" alt="CVPR2020" style="border-style: none" width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Beyond Tracking: Selecting Memeory and Refining Poses for Deep Visual Ododmetry</papertitle>
            <br>
            <strong>Fei Xue</strong>,
            Xin Wang, Shunkai Li, Qiuyuan Wang, Junqiu Wang, Hongbin Zha
            <br>
            <em>CVPR 2019 (oral)</em>
            <br>
            <a href="https://arxiv.org/pdf/1904.01892.pdf">Paper</a> &nbsp/&nbsp
            <a href="https://feixue94.github.io">Code</a> 
            <!-- <a href="https://www.youtube.com/watch?v=w5b8RA3BnRc">Video</a> &nbsp/&nbsp -->
            <!-- <a href="papers/LBR-CVPR2022/slides.pdf">Slides</a> &nbsp/&nbsp -->
            <!-- <a href="papers/LBR-CVPR2022/poster.pdf">Poster</a> -->
            <!-- <a href="https://feixue94.github.io/pram-project/">Project</a> -->
            <br>
          </td>
          </tr>

          <!-- Guided -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="papers/guided.png" alt="ACCV2019" style="border-style: none" width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Guided Feature Selection for Deep Visual Odometry</papertitle>
            <br>
            <strong>Fei Xue</strong>,
            Qiuyuan Wang, Xin Wang, Wei Dong, Junqiu Wang, Hongbin Zha
            <br>
            <em>ACCV 2019</em>
            <br>
            <a href="https://arxiv.org/pdf/1811.09935.pdf">Paper</a> &nbsp/&nbsp
            <a href="https://feixue94.github.io">Code</a> 
            <!-- <a href="https://www.youtube.com/watch?v=w5b8RA3BnRc">Video</a> &nbsp/&nbsp -->
            <!-- <a href="papers/LBR-CVPR2022/slides.pdf">Slides</a> &nbsp/&nbsp -->
            <!-- <a href="papers/LBR-CVPR2022/poster.pdf">Poster</a> -->
            <!-- <a href="https://feixue94.github.io/pram-project/">Project</a> -->
            <br>
          </td>
          </tr>

          <!-- Continuous -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="papers/continuous.png" alt="ACCV2019" style="border-style: none" width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Continuous-time Stereo Visual Odometry Based on Dynamics Model</papertitle>
            <br>
            
            Xin Wang, 
            <strong>Fei Xue</strong>,
            Qiuyuan Wang, Xin Wang, Wei Dong, Junqiu Wang, Hongbin Zha
            <br>
            <em>ACCV 2019</em>
            <br>
            <a href="https://www.researchgate.net/publication/332103736_Continuous-time_Stereo_Visual_Odometry_Based_on_Dynamics_Model">Paper</a> &nbsp/&nbsp
            <a href="https://feixue94.github.io">Code</a> 
            <!-- <a href="https://www.youtube.com/watch?v=w5b8RA3BnRc">Video</a> &nbsp/&nbsp -->
            <!-- <a href="papers/LBR-CVPR2022/slides.pdf">Slides</a> &nbsp/&nbsp -->
            <!-- <a href="papers/LBR-CVPR2022/poster.pdf">Poster</a> -->
            <!-- <a href="https://feixue94.github.io/pram-project/">Project</a> -->
            <br>
          </td>
          </tr>

        </tbody></table>        

      </td>
    </tr>
  </table>



  <br>
  Fei Xue @2023 &nbsp;&nbsp;&nbsp;&nbsp; Total Visitors:
      <a href='https://www.counter12.com'><img src='https://www.counter12.com/img-B9DBC3z0yybdxAYZ-1.gif' border='0' alt='counter'></a><script type='text/javascript' src='https://www.counter12.com/ad.js?id=B9DBC3z0yybdxAYZ'></script>
      &nbsp;&nbsp;&nbsp;&nbsp;
  <a href='https://clustrmaps.com/site/1ar8t'  title='Visit tracker'><img src='//clustrmaps.com/map_v2.png?cl=ffffff&w=a&t=n&d=kB9wZyzFc5Gb5zgk68t8eroVhCtAUrAApYOC1xtKZvw&co=2d78ad&ct=ffffff'/></a>

  <br>
  Thanks to <a href="https://jonbarron.info/">Jon Barron</a> for the website template.
</body>

</html>
